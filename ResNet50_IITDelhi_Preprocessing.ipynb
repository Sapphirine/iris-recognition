{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-precessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_mask(img):\n",
    "    img_org_gray= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    img_histEqualization= cv2.equalizeHist(img_org_gray)\n",
    "    _,img_binary= cv2.threshold(img_org_gray,70,255,cv2.THRESH_BINARY)\n",
    "    center_pupil = cv2.HoughCircles(img_binary, cv2.HOUGH_GRADIENT,1,250,\n",
    "                           param1=80,param2=10,minRadius=30,\n",
    "                           maxRadius=70)\n",
    "    center_pupil = np.uint16(np.around(center_pupil))\n",
    "    draw = img.copy()\n",
    "#     for i in center_pupil[0,:]:\n",
    "#         print(i,i[0],i[1],i[2])\n",
    "#         draw = cv2.circle(img,(i[0],i[1]),i[2],(255,0,0),2)\n",
    "#         draw = cv2.circle(draw,(i[0],i[1]),2,(255,0,0),3)\n",
    "    (x1, y1) = center_pupil[0][0][0], center_pupil[0][0][1]\n",
    "    circles = cv2.HoughCircles(img_histEqualization, cv2.HOUGH_GRADIENT,1,250,\n",
    "                           param1=80,param2=30,minRadius=50,\n",
    "                           maxRadius=100)\n",
    "    circles = np.uint16(np.around(circles))\n",
    "    draw_shift = draw.copy()     \n",
    "    for i in circles[0,:]:       \n",
    "        print(i,i[0],i[1],i[2])\n",
    "        mask = np.zeros_like(img)\n",
    "        mask = cv2.circle(mask, (x1,y1), i[2], (255,255,255), -1)\n",
    "        draw_shift = cv2.circle(draw_shift,(x1,y1),i[2],(255,0,0),2)\n",
    "        #draw_shift = cv2.circle(draw_shift,(x1,y1),2,(255,0,0),3)  \n",
    "        draw_mask = cv2.bitwise_and(draw_shift, mask)\n",
    "    return draw_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = np.zeros([2688, 224, 224,3]), np.zeros([2688])\n",
    "x_test, y_test = np.zeros([896, 224, 224,3]), np.zeros([896])\n",
    "\n",
    "folder = \"IITD_database/people\"\n",
    "i = 0\n",
    "for people in range(1,225):\n",
    "    person_folder = folder + \"/\" + \"0\"*(3-len(str(people))) + str(people)\n",
    "    file_names = os.listdir(person_folder)\n",
    "    for num_image in range(1,7):\n",
    "        path = person_folder + \"/\" + file_names[num_image-1]\n",
    "        image = Image.open(path)\n",
    "        image = image.resize((224,224))\n",
    "        image = np.asarray(image)\n",
    "        image = iris_mask(image)\n",
    "        #x = image.reshape((1,224,224,3))\n",
    "        row_num = i*6 + num_image -1\n",
    "        x_train[row_num,:,:,:] = image\n",
    "        y_train[row_num] = people\n",
    "        \n",
    "        flip = np.flip(x_train[i],1)\n",
    "        flip = np.resize(flip, (1,224,224,3))\n",
    "        x_train[row_num+1344,:,:,:] = flip\n",
    "        y_train[row_num+1344] = people\n",
    "    i += 1\n",
    "    \n",
    "i = 0\n",
    "for people in range(1,225):\n",
    "    person_folder = folder + \"/\" + \"0\"*(3-len(str(people))) + str(people)\n",
    "    file_names = os.listdir(person_folder)\n",
    "    for num_image in range(7,11):\n",
    "        path = person_folder + \"/\" + file_names[num_image-1]\n",
    "        image = Image.open(path)\n",
    "        image = image.resize((224,224))\n",
    "        image = np.asarray(image)\n",
    "        image = iris_mask(image)\n",
    "        #x = image.reshape((1,224,224,3))\n",
    "        row_num = i*4 + num_image -7\n",
    "        x_test[row_num,:,:,:] = image\n",
    "        y_test[row_num] = people\n",
    "    i += 1\n",
    "    \n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = np.random.permutation(2688)\n",
    "index_test = np.random.permutation(896)\n",
    "x_train, y_train = x_train[index_train], y_train[index_train]\n",
    "x_test, y_test = x_test[index_test], y_test[index_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = tf.cast(x_train, tf.float32)/255.0, tf.cast(x_test, tf.float32)/255.0\n",
    "y_train, y_test = tf.one_hot(y_train-1, depth=224), tf.one_hot(y_test-1, depth=224)\n",
    "#y_train, y_test = tf.cast(y_train, tf.float32), tf.cast(y_test, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model (Functional)           (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 224)               458976    \n",
      "=================================================================\n",
      "Total params: 24,046,688\n",
      "Trainable params: 458,976\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "pre_trained_model = ResNet50(input_shape=(224,224,3), weights='imagenet')\n",
    "ResNet = Model(inputs=pre_trained_model.input,outputs=pre_trained_model.layers[-2].output)\n",
    "#ResNet.summary()\n",
    "\n",
    "# First time, train without adjusting the ResNet weights to train the final dense layer first\n",
    "ResNet.trainable =  False\n",
    "model = tf.keras.Sequential([\n",
    "                             ResNet,\n",
    "                             tf.keras.layers.Dense(224, activation='softmax')\n",
    "                                      ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_loss(lambda: 0.01*tf.reduce_sum(tf.square(model.layers[-1].kernel)))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0002),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              metrics=tf.keras.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 35s 243ms/step - loss: 9.0159 - categorical_accuracy: 0.0051 - val_loss: 7.1276 - val_categorical_accuracy: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f68c76b13c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=24, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "90/90 [==============================] - 61s 600ms/step - loss: 7.9444 - categorical_accuracy: 0.1652 - val_loss: 7.9459 - val_categorical_accuracy: 0.0019\n",
      "Epoch 2/15\n",
      "90/90 [==============================] - 52s 578ms/step - loss: 4.2000 - categorical_accuracy: 0.7466 - val_loss: 7.5499 - val_categorical_accuracy: 0.0093\n",
      "Epoch 3/15\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 2.4172 - categorical_accuracy: 0.9733 - val_loss: 7.1274 - val_categorical_accuracy: 0.0056\n",
      "Epoch 4/15\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 1.7100 - categorical_accuracy: 0.9975 - val_loss: 6.7218 - val_categorical_accuracy: 0.0037\n",
      "Epoch 5/15\n",
      "90/90 [==============================] - 52s 577ms/step - loss: 1.2722 - categorical_accuracy: 0.9997 - val_loss: 6.3804 - val_categorical_accuracy: 0.0074\n",
      "Epoch 6/15\n",
      "90/90 [==============================] - 52s 577ms/step - loss: 0.9752 - categorical_accuracy: 1.0000 - val_loss: 6.0908 - val_categorical_accuracy: 0.0186\n",
      "Epoch 7/15\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.7530 - categorical_accuracy: 1.0000 - val_loss: 5.7494 - val_categorical_accuracy: 0.0167\n",
      "Epoch 8/15\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.5926 - categorical_accuracy: 1.0000 - val_loss: 5.3612 - val_categorical_accuracy: 0.0502\n",
      "Epoch 9/15\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.4847 - categorical_accuracy: 0.9993 - val_loss: 4.7880 - val_categorical_accuracy: 0.1059\n",
      "Epoch 10/15\n",
      "90/90 [==============================] - 52s 577ms/step - loss: 0.4065 - categorical_accuracy: 1.0000 - val_loss: 3.7147 - val_categorical_accuracy: 0.3253\n",
      "Epoch 11/15\n",
      "90/90 [==============================] - 52s 577ms/step - loss: 0.3574 - categorical_accuracy: 0.9968 - val_loss: 3.1591 - val_categorical_accuracy: 0.4033\n",
      "Epoch 12/15\n",
      "90/90 [==============================] - 52s 577ms/step - loss: 0.3020 - categorical_accuracy: 0.9989 - val_loss: 1.8044 - val_categorical_accuracy: 0.7528\n",
      "Epoch 13/15\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.2609 - categorical_accuracy: 1.0000 - val_loss: 1.1761 - val_categorical_accuracy: 0.8271\n",
      "Epoch 14/15\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.3499 - categorical_accuracy: 0.9864 - val_loss: 6.0042 - val_categorical_accuracy: 0.0353\n",
      "Epoch 15/15\n",
      "90/90 [==============================] - 52s 575ms/step - loss: 1.5389 - categorical_accuracy: 0.7483 - val_loss: 3.2674 - val_categorical_accuracy: 0.4219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f68cfe6a710>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResNet.trainable =  True\n",
    "model.add_loss(lambda: 0.01*tf.reduce_sum(tf.square(model.layers[-1].kernel)))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0002),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              metrics=tf.keras.metrics.CategoricalAccuracy())\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=24, epochs=15, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "90/90 [==============================] - 52s 578ms/step - loss: 0.6264 - categorical_accuracy: 0.9572 - val_loss: 1.3952 - val_categorical_accuracy: 0.8030\n",
      "Epoch 2/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.3605 - categorical_accuracy: 0.9986 - val_loss: 0.6803 - val_categorical_accuracy: 0.9349\n",
      "Epoch 3/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.2684 - categorical_accuracy: 1.0000 - val_loss: 0.5237 - val_categorical_accuracy: 0.9572\n",
      "Epoch 4/20\n",
      "90/90 [==============================] - 52s 577ms/step - loss: 0.2325 - categorical_accuracy: 0.9991 - val_loss: 0.5121 - val_categorical_accuracy: 0.9480\n",
      "Epoch 5/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.1982 - categorical_accuracy: 1.0000 - val_loss: 0.4729 - val_categorical_accuracy: 0.9461\n",
      "Epoch 6/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.1955 - categorical_accuracy: 0.9977 - val_loss: 0.5134 - val_categorical_accuracy: 0.9405\n",
      "Epoch 7/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.1700 - categorical_accuracy: 1.0000 - val_loss: 0.4250 - val_categorical_accuracy: 0.9517\n",
      "Epoch 8/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.1570 - categorical_accuracy: 1.0000 - val_loss: 0.4050 - val_categorical_accuracy: 0.9498\n",
      "Epoch 9/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.1504 - categorical_accuracy: 1.0000 - val_loss: 0.4120 - val_categorical_accuracy: 0.9480\n",
      "Epoch 10/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.1390 - categorical_accuracy: 1.0000 - val_loss: 0.3933 - val_categorical_accuracy: 0.9572\n",
      "Epoch 11/20\n",
      "90/90 [==============================] - 52s 577ms/step - loss: 0.1319 - categorical_accuracy: 1.0000 - val_loss: 0.3754 - val_categorical_accuracy: 0.9554\n",
      "Epoch 12/20\n",
      "90/90 [==============================] - 52s 577ms/step - loss: 0.1259 - categorical_accuracy: 1.0000 - val_loss: 0.3737 - val_categorical_accuracy: 0.9572\n",
      "Epoch 13/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.1200 - categorical_accuracy: 1.0000 - val_loss: 0.3693 - val_categorical_accuracy: 0.9554\n",
      "Epoch 14/20\n",
      "90/90 [==============================] - 52s 575ms/step - loss: 0.1189 - categorical_accuracy: 1.0000 - val_loss: 0.3774 - val_categorical_accuracy: 0.9517\n",
      "Epoch 15/20\n",
      "90/90 [==============================] - 52s 576ms/step - loss: 0.1184 - categorical_accuracy: 1.0000 - val_loss: 0.4177 - val_categorical_accuracy: 0.9461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f68cc21b908>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "model.fit(x_train, y_train, batch_size=24, epochs=20, validation_split=0.2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 8s 207ms/step - loss: 0.4521 - categorical_accuracy: 0.9420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.45205608010292053, 0.9419642686843872]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
